{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from modules.Attention import DotAttention\n",
    "\n",
    "\n",
    "def test_attention(N:int,D:int,D_k:int,D_v:int,Attention_function):\n",
    "    X = torch.rand([N,D])\n",
    "    WQ = torch.rand([D,D_k])\n",
    "    WK = torch.rand([D,D_k])\n",
    "    WV = torch.rand([D,D_v])\n",
    "\n",
    "    Q = X@WQ\n",
    "    K = X@WK\n",
    "    V = X@WV\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    att = Attention_function(Q,K,V)\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    dot_attention = DotAttention()(Q,K,V)\n",
    "    \n",
    "    return (end-start),F.mse_loss(att,dot_attention).item()\n",
    "\n",
    "def average_test(N:int,D:int,D_k:int,D_v:int,Attention_function,iterations):\n",
    "    total_sum_duration = 0\n",
    "    total_sum_loss = 0\n",
    "    for i in range(iterations):\n",
    "        ret_met = test_attention(N,D,D_k,D_v,Attention_function)\n",
    "        total_sum_duration+=ret_met[0]\n",
    "        total_sum_loss+=ret_met[1]\n",
    "        \n",
    "    print(f\"Average Duration = {(total_sum_duration/iterations):.9f}\\nAverage Loss = {total_sum_loss/iterations}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X \\in \\mathbb{R}^{n \\times d}$, $W_Q \\in \\mathbb{R}^{d \\times d_k}$, $W_k \\in \\mathbb{R}^{d \\times d_k}$, $W_V \\in \\mathbb{R}^{d \\times d_v}$ such that $$\\text{self-attention}(X) = \\sigma\\left(\\frac{(XW_K)^T \\cdot XW_Q}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Duration = 0.000070139\n",
      "Average Loss = 0.0\n"
     ]
    }
   ],
   "source": [
    "from modules.Attention import DotAttention\n",
    "\n",
    "dot_attention = DotAttention()\n",
    "\n",
    "N = 10\n",
    "D = 20\n",
    "D_k = 10\n",
    "D_v = N\n",
    "\n",
    "average_test(N,D,D_k,D_v,dot_attention,100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernal Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same dimensionality as before after the matmult\n",
    "$$\\text{Kernal self attention} = \\frac{\\left(\\phi({Q})\\cdot\\phi({K})^T\\right) V}{\\left(\\phi({Q})\\cdot\\phi({K})^T\\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Duration = 0.000026162\n",
      "Average Loss = 2179.9406811523436\n"
     ]
    }
   ],
   "source": [
    "from modules.Attention import KernalAttention\n",
    "\n",
    "kernal_attention = KernalAttention()\n",
    "\n",
    "average_test(N,D,D_k,D_k,kernal_attention,100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Efficient Attention\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$X \\in \\mathbb{R}^{n \\times d}$, $V \\in \\mathbb{R}^{n \\times d_v}, K \\in \\mathbb{R}^{n \\times d_k}, Q \\in \\mathbb{R}^{n \\times d_k} $ $$\\rho_q(Y) = \\rho_q(Y) = \\frac{Y}{\\sqrt{n}}$$ \n",
    "and, $$\\rho_q(Y) = \\sigma_{\\text{row}} (Y), $$ $$\\rho_k(Y) = \\sigma_{\\text{col}}(Y)$$\n",
    " $$\\text{EffAtt}(X) = \\rho_q(Q) \\cdot \\left(\\rho_k(K)^TV\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Duration = 0.000054338\n",
      "Average Loss = 1.9434287661314011\n"
     ]
    }
   ],
   "source": [
    "from modules.Attention import EfficientAttention\n",
    "\n",
    "efficient_attention = EfficientAttention()\n",
    "\n",
    "average_test(N,D,D_k,D_k,efficient_attention,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Headed Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (8) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m V = X\u001b[38;5;129m@WV\u001b[39m\n\u001b[32m     17\u001b[39m MHA = MultiHeadSelfAttention(\u001b[32m8\u001b[39m,X.size(),D_k,dot_attention)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mMHA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/autoimmenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/autoimmenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/YangLab/AutoImmune-Challenge/modules/Attention.py:104\u001b[39m, in \u001b[36mMultiHeadSelfAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    102\u001b[39m V = torch.matmul(x,\u001b[38;5;28mself\u001b[39m.WV)\n\u001b[32m    103\u001b[39m attention = EfficientAttention()\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m attention_out = \u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m attention_out = attention_out.squeeze(\u001b[32m0\u001b[39m)\n\u001b[32m    106\u001b[39m attention_out = torch.matmul(attention_out,\u001b[38;5;28mself\u001b[39m.WO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/autoimmenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/autoimmenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/YangLab/AutoImmune-Challenge/modules/Attention.py:38\u001b[39m, in \u001b[36mEfficientAttention.forward\u001b[39m\u001b[34m(self, Q, K, V)\u001b[39m\n\u001b[32m     35\u001b[39m     Y = F.softmax(Y,dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Y\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m rho_q(Q)@(\u001b[43mrho_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;129;43m@V\u001b[39;49m)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (10) must match the size of tensor b (8) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from modules.Attention import MultiHeadSelfAttention\n",
    "\n",
    "X = torch.rand([N,D])\n",
    "WQ = torch.rand([D,D_k])\n",
    "WK = torch.rand([D,D_k])\n",
    "WV = torch.rand([D,D_v])\n",
    "\n",
    "Q = X@WQ\n",
    "K = X@WK\n",
    "V = X@WV\n",
    "\n",
    "MHA = MultiHeadSelfAttention(8,X.size(),D_k,dot_attention)\n",
    "MHA(X)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoimmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
